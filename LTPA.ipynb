{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LTPA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-szKRa72U-G",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": "from google.colab import drive\ndrive.mount(\u0027/content/drive\u0027)",
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\nimport os\nimport argparse\nfrom time import gmtime, strftime\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn.functional as F\nimport torchvision.utils as utils\n\nimport logging\nfrom typing import List\nfrom termcolor import colored\n\nos.environ[\u0027CUDA_LAUNCH_BLOCKING\u0027] \u003d \"1\"\n\n#ROOT_DIR \u003d os.path.dirname(os.path.abspath(__file__))\nROOT_DIR \u003d \u0027/content/drive/My Drive/Colab Notebooks\u0027\n#ROOT_DIR \u003d \u0027/content/sample_data/LTPA\u0027\nvgg_in_channels \u003d 3\n\n# define layers for VGG, \u0027M\u0027 for max pooling\nvgg_config \u003d {\n    \u0027VGG11\u0027: [64, \u0027M\u0027, 128, \u0027M\u0027, 256, 256, \u0027M\u0027, 512, 512, \u0027M\u0027, 512, 512, \u0027M\u0027],\n    \u0027VGG13\u0027: [64, 64, \u0027M\u0027, 128, 128, \u0027M\u0027, 256, 256, \u0027M\u0027, 512, 512, \u0027M\u0027, 512, 512, \u0027M\u0027],\n    \u0027VGG16\u0027: [64, 64, \u0027M\u0027, 128, 128, \u0027M\u0027, 256, 256, 256, \u0027M\u0027, 512, 512, 512, \u0027M\u0027, 512, 512, 512, \u0027M\u0027],\n    \u0027VGG19\u0027: [64, 64, \u0027M\u0027, 128, 128, \u0027M\u0027, 256, 256, 256, 256, \u0027M\u0027, 512, 512, 512, 512, \u0027M\u0027, 512, 512, 512, 512, \u0027M\u0027],\n    \u0027VGGAttention\u0027: [64, 64, 128, 128, 256, 256, 256, \u0027M\u0027, 512, 512, 512, \u0027M\u0027, 512, 512, 512, \u0027M\u0027, 512, \u0027M\u0027, 512, \u0027M\u0027],\n}\n\nclass ProjectLogger:\n    def __init__(self,\n                 log_file: str \u003d None,\n                 level: int \u003d logging.DEBUG,\n                 printing: bool \u003d True, attrs: List[str] \u003d None,\n                 name: str \u003d \u0027project_logger\u0027,\n                 ):\n        \"\"\" Basic logger that can write to a file on disk or to sterr.\n        :param log_file: name of the file to log to\n        :param level: logging verbosity level\n        :param printing: flag for whether to log to sterr\n        \"\"\"\n        root_logger \u003d logging.getLogger(name)\n        root_logger.setLevel(level)\n        self.printing \u003d printing\n        self.attrs \u003d attrs\n\n        # Set up writing to a file\n        if log_file:\n            file_handler \u003d logging.FileHandler(log_file, mode\u003d\u0027a\u0027)\n            file_formatter \u003d logging.Formatter(\n                \u0027%(levelname)s: %(asctime)s %(message)s\u0027,\n                datefmt\u003d\u0027%m/%d/%Y %image:%M:%S %p\u0027\n            )\n            file_handler.setFormatter(file_formatter)\n            root_logger.addHandler(file_handler)\n\n        # Set up printing to stderr\n        def check_if_sterr(hdlr: logging.Handler):\n            return isinstance(hdlr, logging.StreamHandler) \\\n                   and not isinstance(hdlr, logging.FileHandler)\n\n        if printing and not list(filter(check_if_sterr, root_logger.handlers)):\n            console_handler \u003d logging.StreamHandler()\n            console_handler.setFormatter(logging.Formatter(\"%(message)s\"))\n            root_logger.addHandler(console_handler)\n\n        self.log \u003d root_logger\n\n    def debug(self, msg, color\u003d\u0027grey\u0027, attrs: List[str] \u003d None):\n        self.log.debug(colored(msg, color, attrs\u003dattrs or self.attrs))\n\n    def info(self, msg, color\u003d\u0027green\u0027, attrs: List[str] \u003d None):\n        self.log.info(colored(msg, color, attrs\u003dattrs or self.attrs))\n\n    def warning(self, msg, color\u003d\u0027blue\u0027, attrs: List[str] \u003d None):\n        self.log.warning(colored(msg, color, attrs\u003dattrs or self.attrs))\n\n    def error(self, msg, color\u003d\u0027magenta\u0027, attrs: List[str] \u003d None):\n        self.log.error(colored(msg, color, attrs\u003dattrs or self.attrs))\n\n    def critical(self, msg, color\u003d\u0027red\u0027, attrs: List[str] \u003d None):\n        self.log.critical(colored(msg, color, attrs\u003dattrs or self.attrs))\n\n\ndef plot_attention(image, attention_estimator, up_factor, nrow):\n    \"\"\"plot attention maps based on attention estimators\"\"\"\n    img \u003d image.permute((1, 2, 0)).cpu().numpy()\n    N, C, W, H \u003d attention_estimator.size()\n    comp_score \u003d torch.softmax(attention_estimator.view(N, C, -1), dim\u003d2).view(\n        N, C, W, H)\n    comp_score \u003d F.interpolate(comp_score, scale_factor\u003dup_factor,\n                               mode\u003d\u0027bilinear\u0027,\n                               align_corners\u003dFalse)\n    attention_img \u003d utils.make_grid(comp_score, nrow\u003dnrow, normalize\u003dTrue,\n                                    scale_each\u003dTrue)\n    attention_img \u003d attention_img.permute((1, 2, 0)).mul(\n        255).byte().cpu().numpy()\n    attention_img \u003d cv2.applyColorMap(attention_img, cv2.COLORMAP_JET)\n    attention_img \u003d cv2.cvtColor(attention_img, cv2.COLOR_BGR2RGB)\n    attention_img \u003d np.float32(attention_img) / 255\n    vis \u003d 0.6 * img + 0.4 * attention_img\n    return torch.from_numpy(vis).permute(2, 0, 1)\n\n\nclass VGGAttention(nn.Module):\n    def __init__(self, mode: str \u003d \u0027pc\u0027):\n        \"\"\"\n        :param mode:\n        dp for dot product for matching the global and local descriptors\n        pc for the use of parametrised compatibility\n        \"\"\"\n        super(VGGAttention, self).__init__()\n        self.mode \u003d mode\n\n        # features through VGG\n        self.features \u003d self._make_layers()\n\n        # right before the 8th, 11th, and 14th layers\n        self.l1 \u003d nn.Sequential(*list(self.features)[:22])\n        self.l2 \u003d nn.Sequential(*list(self.features)[22:32])\n        self.l3 \u003d nn.Sequential(*list(self.features)[32:42])\n\n        # remaining layers before fully-connected\n        self.conv_remain \u003d nn.Sequential(*list(self.features)[42:50])\n\n        # 1st fully-connected back to attention estimator layers\n        self.fc1 \u003d nn.Linear(512, 512)\n        self.ga1 \u003d nn.Linear(512, 256)\n        self.ga2 \u003d nn.Linear(512, 512)\n        self.ga3 \u003d nn.Linear(512, 512)\n\n        # last fully-connected after weight combinations\n        self.fc2 \u003d nn.Linear(256 + 512 + 512, 10)\n\n        if mode \u003d\u003d \u0027pc\u0027:\n            self.u1 \u003d nn.Conv2d(256, 1, 1)\n            self.u2 \u003d nn.Conv2d(512, 1, 1)\n            self.u3 \u003d nn.Conv2d(512, 1, 1)\n\n    def forward(self, x):\n        l1 \u003d self.l1(x)\n        l2 \u003d self.l2(l1)\n        l3 \u003d self.l3(l2)\n        conv_remain \u003d self.conv_remain(l3)\n\n        fc1 \u003d self.fc1(conv_remain.view(conv_remain.size(0), -1))\n        ga1 \u003d self.ga1(fc1)\n        ga2 \u003d self.ga2(fc1)\n        ga3 \u003d self.ga3(fc1)\n\n        ae1 \u003d self._get_compatibility_score(l1, ga1, level\u003d1)\n        ae2 \u003d self._get_compatibility_score(l2, ga2, level\u003d2)\n        ae3 \u003d self._get_compatibility_score(l3, ga3, level\u003d3)\n\n        g1 \u003d self._get_weighted_combination(l1, ae1)\n        g2 \u003d self._get_weighted_combination(l2, ae2)\n        g3 \u003d self._get_weighted_combination(l3, ae3)\n\n        g \u003d torch.cat((g1, g2, g3), dim\u003d1)\n        out \u003d self.fc2(g)\n\n        # need the attention estimators for the image plots\n        return [out, ae1, ae2, ae3]\n\n    @staticmethod\n    def _make_layers():\n        \"\"\"the making of convolutional layers for any VGG architecture\"\"\"\n        layers \u003d []\n        in_channels \u003d vgg_in_channels\n        for x in vgg_config[\u0027VGGAttention\u0027]:\n            if x \u003d\u003d \u0027M\u0027:\n                layers +\u003d [nn.MaxPool2d(kernel_size\u003d2, stride\u003d2)]\n            else:\n                layers +\u003d [nn.Conv2d(in_channels, x, kernel_size\u003d3, padding\u003d1),\n                           nn.BatchNorm2d(x),\n                           nn.ReLU(inplace\u003dTrue)]\n                in_channels \u003d x\n        layers +\u003d [nn.AvgPool2d(kernel_size\u003d1, stride\u003d1)]\n        return nn.Sequential(*layers)\n\n    def _get_compatibility_score(self, l, g, level):\n        \"\"\"secret sauce from the paper\"\"\"\n        if self.mode \u003d\u003d \u0027dp\u0027:\n            ae \u003d l * g.unsqueeze(2).unsqueeze(3)\n            ae \u003d ae.sum(1).unsqueeze(1)\n            size \u003d ae.size()\n            ae \u003d ae.view(ae.size(0), ae.size(1), -1)\n            ae \u003d torch.softmax(ae, dim\u003d2)\n            ae \u003d ae.view(size)\n\n        elif self.mode \u003d\u003d \u0027pc\u0027:\n            ae \u003d l + g.unsqueeze(2).unsqueeze(3)\n            if level \u003d\u003d 1:\n                u \u003d self.u1\n            elif level \u003d\u003d 2:\n                u \u003d self.u2\n            elif level \u003d\u003d 3:\n                u \u003d self.u3\n            ae \u003d u(ae)\n            size \u003d ae.size()\n            ae \u003d ae.view(ae.size(0), ae.size(1), -1)\n            ae \u003d F.softmax(ae, dim\u003d2)\n            ae \u003d ae.view(size)\n        return ae\n\n    @staticmethod\n    def _get_weighted_combination(l, ae):\n        g \u003d l * ae\n        return g.view(g.size(0), g.size(1), -1).sum(2)\n\n\nclass AttentionNetwork:\n    def __init__(self,\n                 opt: argparse.ArgumentParser,\n                 model: torch.nn.Module,\n                 criterion: torch.nn,\n                 optimizer: optim.Optimizer,\n                 scheduler: lr_scheduler.LambdaLR,\n                 device: torch.device,\n                 early_stop: int \u003d 3,\n                 loglevel: int \u003d 20,\n                 ):\n        self.opt \u003d opt\n        self.params \u003d vars(self).copy()\n        self.model \u003d model\n        self.criterion \u003d criterion\n        self.optimizer \u003d optimizer\n        self.scheduler \u003d scheduler\n        self.device \u003d device\n        self.writer \u003d SummaryWriter(\n            log_dir\u003df\u0027{ROOT_DIR}/{opt.logs_path}/tensorboard/\u0027\n                    f\u0027{strftime(\"%Y-%m-%d\", gmtime())}/\u0027\n                    f\u0027{str(self.params)}_{strftime(\"%Y-%m-%d %H-%M-%S\", gmtime())}\u0027)\n        self.logger \u003d ProjectLogger(level\u003dloglevel)\n        self.images \u003d []\n        self.image_dim \u003d int(5 * 5)\n        self.step \u003d 0\n        self.min_up_factor \u003d 2\n        self.early_stop \u003d early_stop\n\n    def train_validate(self,\n                       train_loader: torch.utils.data.DataLoader,\n                       test_loader: torch.utils.data.DataLoader):\n        best_test_loss \u003d float(\u0027inf\u0027)\n        for epoch in range(self.opt.epochs):\n            epoch_train_loss \u003d self.train(train_loader\u003dtrain_loader, epoch\u003depoch)\n            epoch_test_loss \u003d self.test(test_loader\u003dtest_loader, epoch\u003depoch)\n            \"\"\"\n            if best_test_loss \u003e epoch_train_loss:\n                best_test_loss \u003d epoch_train_loss\n                patience \u003d 1\n            else:\n                patience +\u003d 1\n            \n            if patience \u003e self.early_stop:\n                break\n            \"\"\"          \n        tb_layout \u003d {\n            \u0027Training\u0027: {\n                \u0027Losses\u0027: [\u0027Multiline\u0027,\n                           [\u0027epoch_train_loss\u0027, \u0027epoch_test_loss\u0027]],\n                \u0027Accuracy\u0027: [\u0027Multiline\u0027,\n                             [\u0027epoch_train_acc\u0027, \u0027epoch_test_acc\u0027]],\n            }\n        }\n        self.writer.add_custom_scalars(tb_layout)\n        self.writer.close()\n        \n    def train(self,\n              train_loader: torch.utils.data.DataLoader,\n              epoch: int):\n        self.writer.add_scalar(\u0027train_learning_rate\u0027,\n                               self.optimizer.param_groups[0][\u0027lr\u0027], epoch)\n        self.logger.info(f\u0027epoch {epoch} completed\u0027)\n        self.scheduler.step()\n        epoch_loss, epoch_acc \u003d [], []\n        for batch_idx, (inputs, targets) in enumerate(train_loader, 0):\n            inputs, targets \u003d inputs.to(self.device), targets.to(\n                self.device)\n            self.model.train()\n            self.model.zero_grad()\n            self.optimizer.zero_grad()\n            if batch_idx \u003d\u003d 0:\n                self.images.append(inputs[0:self.image_dim, :, :, :])\n            pred, _, _, _ \u003d self.model(inputs)\n            loss \u003d self.criterion(pred, targets)\n            loss.backward()\n            self.optimizer.step()\n            predict \u003d torch.argmax(pred, 1)\n            total \u003d targets.size(0)\n            correct \u003d torch.eq(predict, targets).cpu().sum().item()\n            acc \u003d correct / total\n            if batch_idx % 10 \u003d\u003d 0:\n                self.logger.info(\n                    f\"[epoch {epoch}][batch_idx {batch_idx}]\"\n                    f\"loss: {round(loss.item(), 4)} \"\n                    f\"accuracy: {100 * acc}% \"\n                )\n            epoch_loss +\u003d [loss.item()]\n            epoch_acc +\u003d [acc]\n            self.step +\u003d 1\n            self.writer.add_scalar(\u0027train_loss\u0027, loss.item(), self.step)\n            self.writer.add_scalar(\u0027train_acc\u0027, acc, self.step)\n\n        # log/add on tensorboard\n        epoch_train_loss \u003d np.mean(epoch_loss, axis\u003d0)\n        epoch_train_acc \u003d np.mean(epoch_acc, axis\u003d0)\n        self.writer.add_scalar(\u0027epoch_train_loss\u0027, epoch_train_loss, epoch)\n        self.writer.add_scalar(\u0027epoch_train_acc\u0027, epoch_train_acc, epoch)\n        self.logger.info(f\"[epoch {epoch}] train_acc: \"\n                         f\"{100 * epoch_train_acc}%\")\n        self.logger.info(f\"[epoch {epoch}] train_loss: \"\n                         f\"{epoch_train_loss}\")\n        self.inputs \u003d inputs\n\n        # save model params\n        os.makedirs(f\u0027{ROOT_DIR}/{opt.logs_path}/model_states\u0027, exist_ok\u003dTrue)\n        torch.save(self.model.state_dict(),\n                   f\u0027{ROOT_DIR}/{opt.logs_path}/model_states/net_epoch_{epoch}.pth\u0027)\n        return epoch_train_loss\n\n    def test(self,\n             test_loader: torch.utils.data.DataLoader,\n             epoch: int):\n        epoch_loss, epoch_acc \u003d [], []\n        self.model.eval()\n        with torch.no_grad():\n            for batch_idx, (inputs, targets) in enumerate(test_loader, 0):\n                inputs, targets \u003d inputs.to(\n                    self.device), targets.to(self.device)\n                if batch_idx \u003d\u003d 0:\n                    self.images.append(\n                        self.inputs[0:self.image_dim, :, :, :])\n                pred, _, _, _ \u003d self.model(inputs)\n                loss \u003d self.criterion(pred, targets)\n                predict \u003d torch.argmax(pred, 1)\n                total \u003d targets.size(0)\n                correct \u003d torch.eq(predict, targets).cpu().sum().item()\n                acc \u003d correct / total\n                epoch_loss +\u003d [loss.item()]\n                epoch_acc +\u003d [acc]\n\n            # log/add on tensorboard\n            epoch_test_loss \u003d np.mean(epoch_loss, axis\u003d0)\n            epoch_test_acc \u003d np.mean(epoch_acc, axis\u003d0)\n            self.writer.add_scalar(\u0027epoch_test_loss\u0027, epoch_test_loss, epoch)\n            self.writer.add_scalar(\u0027epoch_test_acc\u0027, epoch_test_acc, epoch)\n            self.logger.info(f\"[epoch {epoch}] test_acc: {100 * epoch_test_acc}%\")\n            self.logger.info(f\"[epoch {epoch}] test_loss: {epoch_test_loss}\")\n\n            # initial image..\n            if epoch \u003d\u003d 0:\n                self.train_image \u003d utils.make_grid(self.images[0],\n                                              nrow\u003dint(np.sqrt(self.image_dim)),\n                                              normalize\u003dTrue, scale_each\u003dTrue)\n                self.test_image \u003d utils.make_grid(self.images[1],\n                                                  nrow\u003dint(\n                                                      np.sqrt(self.image_dim)),\n                                                  normalize\u003dTrue,\n                                                  scale_each\u003dTrue)\n                self.writer.add_image(\u0027train_image\u0027, self.train_image, epoch)\n                self.writer.add_image(\u0027test_image\u0027, self.test_image, epoch)\n\n            # training image sets\n            __, ae1, ae2, ae3 \u003d self.model(self.images[0])\n            attn1 \u003d plot_attention(self.train_image, ae1,\n                                   up_factor\u003dself.min_up_factor,\n                                   nrow\u003dint(np.sqrt(self.image_dim)))\n            self.writer.add_image(\u0027train_attention_map_1\u0027, attn1,\n                                  epoch)\n\n            attn2 \u003d plot_attention(self.train_image, ae2,\n                                   up_factor\u003dself.min_up_factor * 2,\n                                   nrow\u003dint(np.sqrt(self.image_dim)))\n            self.writer.add_image(\u0027train_attention_map_2\u0027, attn2,\n                                  epoch)\n\n            attn3 \u003d plot_attention(self.train_image, ae3,\n                                   up_factor\u003dself.min_up_factor * 4,\n                                   nrow\u003dint(np.sqrt(self.image_dim)))\n            self.writer.add_image(\u0027train_attention_map_3\u0027, attn3,\n                                  epoch)\n\n            # validation image sets\n            __, ae1, ae2, ae3 \u003d self.model(self.images[1])\n            attn1 \u003d plot_attention(self.test_image, ae1,\n                                   up_factor\u003dself.min_up_factor,\n                                   nrow\u003dint(np.sqrt(self.image_dim)))\n            self.writer.add_image(\u0027test_attention_map_1\u0027, attn1,\n                                  epoch)\n            attn2 \u003d plot_attention(self.test_image, ae2,\n                                   up_factor\u003dself.min_up_factor * 2,\n                                   nrow\u003dint(np.sqrt(self.image_dim)))\n            self.writer.add_image(\u0027test_attention_map_2\u0027, attn2,\n                                  epoch)\n            attn3 \u003d plot_attention(self.test_image, ae3,\n                                   up_factor\u003dself.min_up_factor * 4,\n                                   nrow\u003dint(np.sqrt(self.image_dim)))\n            self.writer.add_image(\u0027test_attention_map_3\u0027, attn3,\n                                  epoch)\n            return epoch_test_loss\n\n\ndef get_data_loader(opt, im_size\u003d32) -\u003e torch.utils.data.DataLoader:\n    transform_train \u003d transforms.Compose([\n        transforms.RandomCrop(im_size, padding\u003d4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n    ])\n    transform_test \u003d transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n    ])\n    train_data \u003d torchvision.datasets.CIFAR10(root\u003df\u0027{ROOT_DIR}/data/CIFAR10\u0027,\n                                              train\u003dTrue,\n                                              download\u003dTrue,\n                                              transform\u003dtransform_train)\n    train_loader \u003d torch.utils.data.DataLoader(train_data,\n                                               batch_size\u003dopt.batch_size,\n                                               shuffle\u003dTrue,\n                                               num_workers\u003dos.cpu_count())\n    test_data \u003d torchvision.datasets.CIFAR10(root\u003df\u0027{ROOT_DIR}/data/CIFAR10\u0027,\n                                             train\u003dFalse,\n                                             download\u003dTrue,\n                                             transform\u003dtransform_test)\n    test_loader \u003d torch.utils.data.DataLoader(test_data,\n                                              batch_size\u003dopt.batch_size,\n                                              shuffle\u003dFalse,\n                                              num_workers\u003dos.cpu_count())\n    return train_loader, test_loader\n\n\nif __name__ \u003d\u003d \u0027__main__\u0027:\n    parser \u003d argparse.ArgumentParser(description\u003d\"LTPA\")\n    parser.add_argument(\"--attention_mode\", type\u003dstr, default\u003d\"pc\",\n                        help\u003d\"mode for running the attention model [pc or dp]\")\n    parser.add_argument(\"--epochs\", type\u003dint, default\u003d300,\n                        help\u003d\"number of epochs\")\n    parser.add_argument(\"--batch_size\", type\u003dint, default\u003d128,\n                        help\u003d\"batch size\")\n    parser.add_argument(\"--lr\", type\u003dfloat, default\u003d0.1,\n                        help\u003d\"initial learning rate\")\n    parser.add_argument(\"--logs_path\", type\u003dstr, default\u003d\"logs\",\n                        help\u003d\u0027path of log files\u0027)\n    opt \u003d parser.parse_args(\u0027\u0027)\n\n    device \u003d torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(999)\n    else:\n        torch.manual_seed(999)\n        torch.set_num_threads(os.cpu_count())\n        print(f\u0027Using {device}: {torch.get_num_threads()} threads\u0027)\n\n    # load data\n    train_loader, test_loader \u003d get_data_loader(opt, im_size\u003d32)\n\n    # model + loss function + optimizer + scheduler\n    net \u003d VGGAttention(mode\u003dopt.attention_mode)\n    criterion \u003d nn.CrossEntropyLoss()\n    if torch.cuda.is_available():\n        model \u003d nn.DataParallel(net, device_ids\u003dlist(\n            range(torch.cuda.device_count()))).to(device)\n    else:\n        model \u003d net.to(device)\n    criterion.to(device)\n    optimizer \u003d optim.SGD(model.parameters(), lr\u003dopt.lr, momentum\u003d0.9,\n                          weight_decay\u003d5e-4)\n    scheduler \u003d lr_scheduler.LambdaLR(optimizer,\n                                      lr_lambda\u003dlambda epoch: np.power(0.5, int(\n                                          epoch / 25)))\n\n    # time to train/validate\n    obj \u003d AttentionNetwork(opt\u003dopt,\n                           model\u003dmodel,\n                           criterion\u003dcriterion,\n                           optimizer\u003doptimizer,\n                           scheduler\u003dscheduler,\n                           device\u003ddevice)\n    obj.train_validate(train_loader\u003dtrain_loader, test_loader\u003dtest_loader)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ]
}